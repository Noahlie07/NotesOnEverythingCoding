# Transformer Models in HF
The ðŸ¤— Transformers library provides the functionality to create and use shared transformer models. HF's Model Hub contains millions of pretrained models that anyone can download and use.  

## Working with HF pipelines
```
from transformers import pipeline
```
The most basic object in the ðŸ¤— Transformers library is the pipeline() function. It connects a model with its necessary preprocessing and postprocessing steps, simplifying the process of using a HF model.  
  
**FYI: The model is downloaded and cached when you create the classifier object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.**  

HF Pipelines work by accepting a task parameter where we state the task we want to conduct. An object is then instantiated from the task-specified pipeline.  
To conduct the task, we pass in all necessary arguments necessary for the task into the object.  

```
# for example, a question-answering pipeline
question_answerer = pipeline("question-answering") # question_answerer object is created
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face in Brooklyn",
) # question_answerer object takes in specific arguments required for its task
```

**Which model will the pipeline use?**  
HF defines a default model for each task. However, we can specify the model we would like to use using the model argument. 
```
# for example
translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
```

## Under the Hood of HF Pipelines  
Pipelines handle all of preprocessing (converting text to numerical representations), passing inputs through the model, and postprocessing (cnverting numerical representations to text).  

### First Step: Preprocessing with a Tokenizer  
The first step inside a pipeline is to preprocess the text using a tokenizer.  
We can either use the default tokenizer defined by the pipeline or define a specific tokenizer to use.  
The process of tokenizing, in detail, involves:
- Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens
- Mapping each token to an integer
- Adding additional inputs that may be useful to the model
Preprocessing needs to be done in exactly the same way as when the model was pretrained. This means we must use the same checkpoint weights as our model.
Below is code for initializing a tokenizer.  
```
# For Example
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```
The tokenizer will take raw input (string or a list of strings), and output a tensor (of size num.of.inputs x vector_space_of_token_vector_representation).  
This is because our model later on will only accept tensors.  
```
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt") #"pt" for pytorch tensors, "np" for numpy tensors
```

### Second Step: Model  
The model takes an input numerical tensor (produced by preprocessing) and generates an output numerical tensor (to be passed on to postprocessing).  
The model needs to be loaded with the same checkpoint as the pre-processing.
```
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```
The vector output by the Transformer module is usually large. It generally has three dimensions:
- Batch size: The number of sequences processed at a time (2 in our example).
- Sequence length: The length of the numerical representation of the sequence (16 in our example).
- Hidden size: The vector dimension of each model input.
```
outputs = model(**inputs)
# print(outputs.last_hidden_state.shape) to see the shape of the model's vector output
```

#### What are model heads and What goes on inside a model?  
A model first takes in embeddings generated by the tokenizer and passes it through its architectural layers. This generates hidden states.  
Hidden states are then passed into the model's head.  
The model's head maps the hidden state to a different dimension (which is essentially the output before decoding).  

Model heads serve different tasks. All can be imported from the transformers library: ex.   
*Model (retrieve the hidden states only)  
*ForCausalLM  
*ForMaskedLM  
*ForMultipleChoice  
*ForQuestionAnswering  
*ForSequenceClassification  
*ForTokenClassification  

```
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```

### Final Step: Post Processing  
```output = model(**inputs)``` produces the output logits, accessible via ```output.logits```.  
Logits are raw, unnormalized values that require postprocessing. The method of postprocessing differs whether the logits are supposed to represent probabilities or tokens.  

#### Example: Logits representing probabilities  
Convert logits to probabilities using softmax.  
```
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```
Run `model.config.id2label` to find out what each class represents.Then, for each output based on the probabilities, we see which class the model predicts it as.  

#### Example: Logits representing tokens  
HF has a built-in generate method within model to generate text immediately.
```
output = model.generate(
    **inputs,
    max_length=100,
    temperature=0.7,
    top_k=50,
    top_p=0.9,
    do_sample=True,  # Enable sampling
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
```

Alternatively, the under-the-hood mechanism of the generate method is as follows:
```
for _ in range(50): # max 50 words
    outputs = model(input_ids)
    next_token_logits = outputs.logits[:, -1, :] / 0.7  # Temperature=0.7
    
    # Top-k filtering (k=50) - select top 50 most likely token values
    top_k = 50
    top_logits, top_indices = next_token_logits.topk(top_k, dim=-1)
    mask = torch.zeros_like(next_token_logits).scatter_(-1, top_indices, top_logits)
    next_token_logits = mask
    
    probs = torch.softmax(next_token_logits, dim=-1)
    next_token = torch.multinomial(probs, num_samples=1)
    
    input_ids = torch.cat([input_ids, next_token], dim=-1)
    
    if next_token == tokenizer.eos_token_id:
        break

generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
print(generated_text)
```

